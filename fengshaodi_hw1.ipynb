{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fengshaodi hw1.ipynb",
      "provenance": [],
      "mount_file_id": "1H2l6izkcayE6Qm5mYN-eWXA56nGAoZuS",
      "authorship_tag": "ABX9TyPJgAthfHP2SRG+f3ki4kDR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fsdfsd123/deep-learning-homework/blob/main/fengshaodi_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYm1IA9Q-vW9",
        "outputId": "2a2128b1-ae39-4785-bf21-7a5cf34752b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%cd drive/My Drive/deep learning/HW1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/My Drive/deep learning/HW1'\n",
            "/content/drive/My Drive/deep learning/HW1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B6aGIz--VBk"
      },
      "source": [
        "import scipy.io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "%matplotlib inline"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRC3JxU2UAHj"
      },
      "source": [
        "question 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUgrwg1kSuX1"
      },
      "source": [
        "train = scipy.io.loadmat('train.mat')\n",
        "test = scipy.io.loadmat('test.mat')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w761JxZUV3e6"
      },
      "source": [
        "X_train = np.append(train['x1'],train['x2'],axis=1)\n",
        "y_train = train['y']\n",
        "X_test = np.append(test['x1'],test['x2'],axis=1)\n",
        "y_test = test['y']"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14NICnSnuYYy"
      },
      "source": [
        "##The sigmoid function adjusts the cost function hypotheses to adjust the algorithm proportionally for worse estimations\n",
        "def Sigmoid(z):\n",
        "\tG_of_Z = float(1.0 / float((1.0 + math.exp(-1.0*z))))\n",
        "\treturn G_of_Z \n",
        "\n",
        "##The hypothesis is the linear combination of all the known factors x[i] and their current estimated coefficients theta[i] \n",
        "##This hypothesis will be used to calculate each instance of the Cost Function\n",
        "def Hypothesis(theta, x):\n",
        "\tz = 0\n",
        "\tfor i in range(len(theta)):\n",
        "\t\tz += x[i]*theta[i]\n",
        "\treturn Sigmoid(z)\n",
        "\n",
        "##For each member of the dataset, the result (Y) determines which variation of the cost function is used\n",
        "##The Y = 0 cost function punishes high probability estimations, and the Y = 1 it punishes low scores\n",
        "##The \"punishment\" makes the change in the gradient of ThetaCurrent - Average(CostFunction(Dataset)) greater\n",
        "def Cost_Function(X,Y,theta,m):\n",
        "\tsumOfErrors = 0\n",
        "\tfor i in range(m):\n",
        "\t\txi = X[i]\n",
        "\t\thi = Hypothesis(theta,xi)\n",
        "\t\tif Y[i] == 1:\n",
        "\t\t\terror = Y[i] * math.log(hi)\n",
        "\t\telif Y[i] == 0:\n",
        "\t\t\terror = (1-Y[i]) * math.log(1-hi)\n",
        "\t\tsumOfErrors += error\n",
        "\tconst = -1/m\n",
        "\tJ = const * sumOfErrors\n",
        "\tprint ('cost is ', J )\n",
        "\treturn J\n",
        "\n",
        "##This function creates the gradient component for each Theta value \n",
        "##The gradient is the partial derivative by Theta of the current value of theta minus \n",
        "##a \"learning speed factor aplha\" times the average of all the cost functions for that theta\n",
        "##For each Theta there is a cost function calculated for each member of the dataset\n",
        "def Cost_Function_Derivative(X,Y,theta,j,m,alpha):\n",
        "\tsumErrors = 0\n",
        "\tfor i in range(m):\n",
        "\t\txi = X[i]\n",
        "\t\txij = xi[j]\n",
        "\t\thi = Hypothesis(theta,X[i])\n",
        "\t\terror = (hi - Y[i])*xij\n",
        "\t\tsumErrors += error\n",
        "\tm = len(Y)\n",
        "\tconstant = float(alpha)/float(m)\n",
        "\tJ = constant * sumErrors\n",
        "\treturn J\n",
        "\n",
        "##For each theta, the partial differential \n",
        "##The gradient, or vector from the current point in Theta-space (each theta value is its own dimension) to the more accurate point, \n",
        "##is the vector with each dimensional component being the partial differential for each theta value\n",
        "def Gradient_Descent(X,Y,theta,m,alpha):\n",
        "\tnew_theta = []\n",
        "\tconstant = alpha/m\n",
        "\tfor j in range(len(theta)):\n",
        "\t\tCFDerivative = Cost_Function_Derivative(X,Y,theta,j,m,alpha)\n",
        "\t\tnew_theta_value = theta[j] - CFDerivative\n",
        "\t\tnew_theta.append(new_theta_value)\n",
        "\treturn new_theta\n",
        "\n",
        "##The high level function for the LR algorithm which, for a number of steps (num_iters) finds gradients which take \n",
        "##the Theta values (coefficients of known factors) from an estimation closer (new_theta) to their \"optimum estimation\" which is the\n",
        "##set of values best representing the system in a linear combination model\n",
        "def Logistic_Regression(X,Y,alpha,theta,num_iters):\n",
        "  m = len(Y)\n",
        "  for x in range(num_iters):\n",
        "    new_theta = Gradient_Descent(X,Y,theta,m,alpha)\n",
        "    theta = new_theta\n",
        "    if x % 100 == 0:\n",
        "      #here the cost function is used to present the final hypothesis of the model in the same form for each gradient-step iteration\n",
        "      Cost_Function(X,Y,theta,m)\n",
        "      print ('theta ', theta)\t\n",
        "      print ('cost is ', Cost_Function(X,Y,theta,m))\n",
        "  return theta\n",
        "\n",
        "##This method compares the accuracy of the model generated by the scikit library with the model generated by this implementation\n",
        "def Test_on_test_data(theta,X_test,y_test):\n",
        "  score = 0\n",
        "  length = len(X_test)\n",
        "  for i in range(length):\n",
        "    prediction = round(Hypothesis(X_test[i],theta))\n",
        "    answer = y_test[i]\n",
        "    if prediction == answer:\n",
        "        score += 1\n",
        "  #the same process is repeated for the implementation from this module and the scores compared to find the higher match-rate\n",
        "  my_score = float(score) / float(length)\n",
        "  print(1-my_score)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACECfeVUObSP",
        "outputId": "1bd0490b-1a5f-4c7d-c90c-6bf6a582c055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        }
      },
      "source": [
        "# These are the initial guesses for theta as well as the learning rate of the algorithm\n",
        "# A learning rate too low will not close in on the most accurate values within a reasonable number of iterations\n",
        "# An alpha too high might overshoot the accurate values or cause irratic guesses\n",
        "# Each iteration increases model accuracy but with diminishing returns, \n",
        "# and takes a signficicant coefficient times O(n)*|Theta|, n = dataset length\n",
        "initial_theta = [0,0]\n",
        "alpha = 0.1\n",
        "iterations = 1000\n",
        "theta = Logistic_Regression(X_train,y_train,alpha,initial_theta,iterations)\n",
        "Test_on_test_data(theta,X_test,y_test)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost is  [0.68510802]\n",
            "theta  [array([-0.02407143]), array([0.0175])]\n",
            "cost is  [0.68510802]\n",
            "cost is  [0.68510802]\n",
            "cost is  [0.32677589]\n",
            "theta  [array([-0.9330396]), array([1.6332142])]\n",
            "cost is  [0.32677589]\n",
            "cost is  [0.32677589]\n",
            "cost is  [0.22138613]\n",
            "theta  [array([-1.43494234]), array([2.52003315])]\n",
            "cost is  [0.22138613]\n",
            "cost is  [0.22138613]\n",
            "cost is  [0.17226289]\n",
            "theta  [array([-1.78000741]), array([3.12753327])]\n",
            "cost is  [0.17226289]\n",
            "cost is  [0.17226289]\n",
            "cost is  [0.14356062]\n",
            "theta  [array([-2.04474072]), array([3.59230231])]\n",
            "cost is  [0.14356062]\n",
            "cost is  [0.14356062]\n",
            "cost is  [0.1245406]\n",
            "theta  [array([-2.26075926]), array([3.97070005])]\n",
            "cost is  [0.1245406]\n",
            "cost is  [0.1245406]\n",
            "cost is  [0.11089581]\n",
            "theta  [array([-2.44404109]), array([4.2911712])]\n",
            "cost is  [0.11089581]\n",
            "cost is  [0.11089581]\n",
            "cost is  [0.10056042]\n",
            "theta  [array([-2.60376774]), array([4.57003871])]\n",
            "cost is  [0.10056042]\n",
            "cost is  [0.10056042]\n",
            "cost is  [0.09241677]\n",
            "theta  [array([-2.74570149]), array([4.81753262])]\n",
            "cost is  [0.09241677]\n",
            "cost is  [0.09241677]\n",
            "cost is  [0.08580546]\n",
            "theta  [array([-2.87369826]), array([5.04048919])]\n",
            "cost is  [0.08580546]\n",
            "cost is  [0.08580546]\n",
            "0.033333333333333326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHA5JAiQhPwZ"
      },
      "source": [
        "question 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9ezfNFfhMUr"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# input image dimensions 28x28\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFrMjgsRhWba"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "amount= 50\n",
        "lines = 5\n",
        "columns = 10\n",
        "number = np.zeros(amount)\n",
        "\n",
        "for i in range(amount):\n",
        "    number[i] = y_test[i]\n",
        "    # print(number[0])\n",
        "\n",
        "fig = plt.figure()\n",
        "randomlist = random.sample(range(1,x_test.shape[0]), amount)\n",
        "for i in range(len(randomlist)):\n",
        "    ax = fig.add_subplot(lines, columns, 1 + i)\n",
        "    plt.imshow(x_test[randomlist[i],:,:], cmap='binary')\n",
        "    plt.sca(ax)\n",
        "    ax.set_xticks([], [])\n",
        "    ax.set_yticks([], [])\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5JI0ziSkwnH"
      },
      "source": [
        "x_train_norm = (x_train-x_train.mean(axis=0))/x_train.std(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qKPgF2qp1Eh"
      },
      "source": [
        "x_train.shape,y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHDJR9Z1n1He"
      },
      "source": [
        "from numpy import linalg as LA"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}